# Multi master node state change test
# This test serves the purpose of evaluating the state changes a node can
# undergo.
# The state changes tested are:
# 1) Single node cluster
#    Node 1: Offline =>  Online
# 2) Node joining into existing cluster
#    Node 2: Offline => Recovering => Online
# 3) Node leaving (after adding another node to guarantee quorum):
#    Node 3: Online => Offline

--source include/have_debug.inc
--source include/have_group_replication_plugin.inc
--let $group_replication_group_name= 89ab83b0-9f17-11e3-a5e2-0800200c9a66
--let $rpl_skip_group_replication_start= 1
--let $rpl_server_count= 3
--source include/master-slave.inc

SET @debug_save= @@GLOBAL.DEBUG;

--echo #
--echo # 1) Setup a new single node cluster
--echo #

--connection server1
--echo server1

--echo # Set the debug flag to block recovery
SET @@GLOBAL.DEBUG='d,recovery_thread_wait';

--echo # Node should be offline before start

--let $assert_text= On the new stopped node, the status is OFFLINE
--let $assert_cond= [select count(*) from performance_schema.replication_group_members WHERE member_state="OFFLINE"] = 1;
--source include/assert.inc

--source include/start_group_replication.inc

--echo # Node should be online as the node is alone and doesn't need recovery

--let $group_replication_member_state= ONLINE
--source include/rpl_group_replication_wait_for_member_state.inc

--echo #
--echo # 2) A new node enters the existing cluster
--echo #

--connection server2
--echo server2

--echo # Set the debug flag to block recovery
SET @@GLOBAL.DEBUG='d,recovery_thread_wait';
SET GLOBAL group_replication_group_name= "89ab83b0-9f17-11e3-a5e2-0800200c9a66";
START GROUP_REPLICATION;

--echo # Node should be marked as on recovery on both nodes

--let $group_replication_member_state= RECOVERING
--source include/rpl_group_replication_wait_for_member_state.inc

--let $group_replication_member_id= query_get_value(SELECT @@SERVER_UUID, @@SERVER_UUID, 1)

--connection server1
--echo server1

--let $group_replication_member_state= RECOVERING
--source include/rpl_group_replication_wait_for_member_state.inc

--connection server2
--echo server2

SET DEBUG_SYNC= "now SIGNAL signal.recovery_continue";

--echo # Node should be marked as online on both nodes

--let $group_replication_member_state= ONLINE
--source include/rpl_group_replication_wait_for_member_state.inc

--connection server1
--echo server1

--let $group_replication_member_state= ONLINE
--source include/rpl_group_replication_wait_for_member_state.inc

--echo #
--echo # 3) A node leaves the existing cluster
--echo #

--echo # Add a new node to hold quorum

--connection server3
--echo server3
--source include/start_group_replication.inc

--echo # Node should be marked as online

#reset the node id variable as it was pointing server 2
--let $group_replication_member_id=
--let $group_replication_member_state= ONLINE
--source include/rpl_group_replication_wait_for_member_state.inc

--source include/stop_group_replication.inc

--echo # Node should be marked as offline after stop

--let $group_replication_member_state= OFFLINE
--source include/rpl_group_replication_wait_for_member_state.inc

--echo #
--echo # Cleaning up
--echo #

--let $server_count=3
while ($server_count)
{
  --connection server$server_count
  --echo server$server_count
  SET @@GLOBAL.DEBUG= @debug_save;
  --dec $server_count
}

--source include/rpl_end.inc
