# This test verifies correctness and stability of WL#7332 implementation
# of State Exchange subprotocol.
#
# The structure is as the following.
#
# Z.   Initialization.
#
# I.   The general stability part when servers leave and join randomly.
#      The purpose is to prove no hangup or crashes.
# II.  Two node cluster that demonstrates some (feasible) of Primary component
#      properties.
# III. Proof of view-id monotonicity and cluster-wide consistency.
#
--source include/have_group_replication_plugin.inc
--let $group_replication_group_name= 8a94f357-aab4-11df-86ab-c80aa9429573
--let $rpl_skip_group_replication_start= 1
--let $rpl_server_count= 4
--source include/master-slave.inc

#
# a counter to be used with source ../inc/rpl_group_replication_error_out.inc
#
--let $group_replication_error_connection_number= 4

#
# First prepare four nodes and manually create a table on each.
#
--connection server1

SET SESSION sql_log_bin= 0;
call mtr.add_suppression("could not form the cluster");
SET SESSION sql_log_bin= 1;

if (`select view_id <> 0 from performance_schema.replication_group_member_stats`)
{
    --echo incorrect non-zero view_id when the Node is never started.
    --source ../inc/rpl_group_replication_error_out.inc
    --die
}

--connection server4

CREATE TABLE t1 (c1 INT NOT NULL PRIMARY KEY) ENGINE=InnoDB;

#
# Part I.
#
# Restart four nodes few times to prove general stability. It should
# be done carefully to not destroy the cluster by occasional
# withdrawal more than cluster.number_of_nodes / 2 - 1 nodes at a
# time, that is not more than 1 in the condition of this test.
# The test implements the following transition: 4 -> 3 -> 2 -> 3 -> 4.
#

--disable_query_log
--disable_result_log
--let $include_silent= 1

--let $s_cnt=4
while($s_cnt)
{
    --connection server$s_cnt
    --source include/start_group_replication.inc

    --dec $s_cnt
}

--let $group_replication_number_of_members= 4
--source ../inc/rpl_group_replication_wait_for_number_of_members.inc

--let $restart_cnt= 10
while($restart_cnt)
{
    --let $s_id1=`select 1 + floor(rand()*100 % 4)`

    --connection server$s_id1

    STOP GROUP_REPLICATION;

    --let $s_id2= $s_id1
    while ($s_id2 == $s_id1)
    {
	--let $s_id2=`select 1 + floor(rand()*100 % 4)`
    }
    --connection server$s_id2

    #
    # Here is the care point: don't go stopping the 2nd node until
    # it's proved the previous one is out of the view.
    #
    --let $group_replication_number_of_members= 3
    --source ../inc/rpl_group_replication_wait_for_number_of_members.inc

    STOP GROUP_REPLICATION;

    --connection server$s_id1
    START GROUP_REPLICATION;

    --connection server$s_id2
    --source include/start_group_replication.inc

    --let $group_replication_number_of_members= 4
    --source ../inc/rpl_group_replication_wait_for_number_of_members.inc

    --dec $restart_cnt
}
--let $include_silent= 0
--enable_result_log
--enable_query_log

#Check if all nodes are online
--let $group_replication_number_of_members= 4
--source ../inc/rpl_group_replication_wait_for_number_of_members.inc

#
# Part II.
#
# Form the 2 node cluster and verify view-id on each node, must be equal.
# At forming make sure two extra members exits one by one so the quorum
# is not get lost.
#

--connection server3
--source include/stop_group_replication.inc

--connection server4
--let $group_replication_number_of_members= 3
--source ../inc/rpl_group_replication_wait_for_number_of_members.inc

--source include/stop_group_replication.inc

--connection server1
--let $group_replication_number_of_members= 2
--source ../inc/rpl_group_replication_wait_for_number_of_members.inc

# end of forming.

--connection server1
INSERT INTO t1 SET c1=1;

--connection server2

#
# Success in getting expected value indicates the Primary Component
# is installed. Let's verify the # of nodes is as PS says
#
let $count= 1;
let $table= t1;
--source include/wait_until_rows_count.inc

if (`SELECT COUNT(*) <> 2 from performance_schema.replication_group_members`)
{
    --echo Unexpected cluster membership.
    --source ../inc/rpl_group_replication_error_out.inc
    --die
}

# By above the Primary Component of two group members installation is proved.
# Let's check out view-id values.
--connection server1
--let $view_id_server1=`select view_id from performance_schema.replication_group_member_stats`

--connection server2
--let $view_id_server2=`select view_id from performance_schema.replication_group_member_stats`

if ($view_id_server1 == 0)
{
    --echo view_id must be non-zero at this point.
    --source ../inc/rpl_group_replication_error_out.inc
    --die
}
if ($view_id_server1 != $view_id_server2)
{
    --echo inconsistent view_id:s: $view_id_server1 != $view_id_server2
    --source ../inc/rpl_group_replication_error_out.inc
    --die
}

#
# Verify the Primary Component property of aborting transactions
# whose originator nodes left the Primary Component.
# The policy is tentative, it might (should) be a subject of
# softer treatment, e.g wait with aborting till a timeout elapses.
# Yet it's tested 'cos it's there.
#
--connection server1
BEGIN;
INSERT INTO t1 SET c1=2;
# Split the cluster and see that the left alone server1 won't make
# Primary Component out of itself.

--connection server2
--source include/stop_group_replication.inc

# TODO: implement COMMIT instead of the coded rollback
--connection server1
rollback;

#
# Part III.
#
# Form the four node cluster, check up monotonic and cluster consistent view-id.
#
--connection server2
--source include/start_group_replication.inc

--connection server3
--source include/start_group_replication.inc

--connection server4
--source include/start_group_replication.inc

--let $s_cnt=4
while ($s_cnt)
{
    --connection server$s_cnt
    --let $group_replication_number_of_members= 4
    --source ../inc/rpl_group_replication_wait_for_number_of_members.inc

    --dec $s_cnt
}
#
#  a proof of view-id is monotonic
#
--let $view_id_now=`select RIGHT(view_id, 2) from performance_schema.replication_group_member_stats`
--let $view_id_server1_number= `SELECT RIGHT('$view_id_server1', 2)`
if ($view_id_now <= $view_id_server1_number)
{
    --echo Unexpected non-increased view-id value.
    --source ../inc/rpl_group_replication_error_out.inc
    --die
}

#
#  view-id cross cluster consistency
#
--let $s_cnt=3
while ($s_cnt)
{
    --connection server$s_cnt
    --let $view_id_now_s=`select RIGHT(view_id,2) from performance_schema.replication_group_member_stats`
    if ($view_id_now != $view_id_now_s)
    {
      --echo The 4th node view inconsistent with that of $s_cnt.
      --source ../inc/rpl_group_replication_error_out.inc
      --die
    }
    dec $s_cnt;
}


#
#  Cleanup
#
DROP TABLE t1;

--source include/rpl_end.inc
