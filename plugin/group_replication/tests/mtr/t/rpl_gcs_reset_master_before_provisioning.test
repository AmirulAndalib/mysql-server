# This test intends to prove that nodes can survive reset master commands.
# These commands end up causing the nodes to purge their relay logs even when
# the node has some provisioned data.
# This test has 3 phases:
#  1) Insert some data and sync both nodes
#  2) Reset both nodes, insert some data on node one
#     Provision node two and start replication
#     Check that all data from phase 1 disappeared and the nodes are in sync.
#  3) Restart group replication on node 2, testing that no data remains
#     from phase 1

--source include/have_group_replication_plugin.inc
--let $group_replication_group_name= c7ce5980-0851-11e4-9191-0800200c9a66
--let $rpl_skip_group_replication_start= 1
--source include/master-slave.inc

#
# Phase 1: Start the cluster and insert data
#

--connection server1
--source include/start_group_replication.inc

#Create a table and insert some data
--connection server1
CREATE TABLE t1 (c1 INT NOT NULL PRIMARY KEY) ENGINE=InnoDB;

--connection server2
--source include/start_group_replication.inc

--connection server1
INSERT INTO t1 VALUES (1);
INSERT INTO t1 VALUES (2);
INSERT INTO t1 VALUES (3);
INSERT INTO t1 VALUES (4);

--let $sync_slave_connection= server2
--source include/sync_slave_sql_with_master.inc

#Both nodes must have the same GTID set
--let $server_count=2
while ($server_count)
{
  --connection server$server_count
  --let $assert_text= On node $server_count, all executed GTID should belong to the cluster
  --let $assert_cond= "[SELECT @@GLOBAL.GTID_EXECUTED]" = "c7ce5980-0851-11e4-9191-0800200c9a66:1-5";
  --source include/assert.inc
  --dec $server_count
}

DROP TABLE t1;

--let $sync_slave_connection= server2
--source include/sync_slave_sql_with_master.inc

#
# Phase 2: Reset the cluster and provision a server
#

#Stop and reset both nodes

--connection server1
--source include/stop_group_replication.inc
RESET MASTER;

--connection server2
--source include/stop_group_replication.inc
RESET MASTER;

--connection server1

CREATE TABLE t1 (c1 INT NOT NULL PRIMARY KEY) ENGINE=InnoDB;
INSERT INTO t1 VALUES (1);

--let $server_uuid= `SELECT @@GLOBAL.SERVER_UUID`

--source include/start_group_replication.inc

--connection server2

#Fake the node being provisioned.

--disable_result_log
--disable_query_log
eval SET @@SESSION.GTID_NEXT= '$server_uuid:1';
CREATE TABLE t1 (c1 INT NOT NULL PRIMARY KEY) ENGINE=InnoDB;
eval SET @@SESSION.GTID_NEXT= '$server_uuid:2';
INSERT INTO t1 VALUES (1);
--enable_result_log
--enable_query_log

SET GTID_NEXT = 'AUTOMATIC';

#Start the node
--source include/start_group_replication.inc

#Test that the cluster works

--connection server1

INSERT INTO t1 VALUES (2);

--connection server2

INSERT INTO t1 VALUES (3);

--connection server1

--let $sync_slave_connection= server2
--source include/sync_slave_sql_with_master.inc

#Both nodes should possess the same data

--let $server_count=2
while ($server_count)
{
  --connection server$server_count
  --let $assert_text= On node $server_count, the table should exist and have 3 elements
  --let $assert_cond= [select count(*) from t1] = 3;
  --source include/assert.inc
  --dec $server_count
}

#Wait for the certification thread to kick in
--sleep 65

if(`SELECT "$server_uuid" < "$group_replication_group_name"`)
{
--let $wait_condition= SELECT transactions_committed_all_members like "$server_uuid:1-2,%$group_replication_group_name:1-2" from performance_schema.replication_group_member_stats;
}
if(`SELECT "$server_uuid" > "$group_replication_group_name"`)
{
--let $wait_condition= SELECT transactions_committed_all_members like "$group_replication_group_name:1-2,%$server_uuid:1-2" from performance_schema.replication_group_member_stats;
}
--source include/wait_condition.inc
--echo [ The value of stable_set should contain both provisioned and cluster GIIDs ]


#
# Phase 3: Restart group replication on node 2
#

--connection server2

--source include/stop_group_replication.inc
--source include/start_group_replication.inc

INSERT INTO t1 VALUES (4);

#Both nodes should possess the same data

--let $server_count=2
while ($server_count)
{
  --connection server$server_count
  --let $assert_text= On node $server_count, the table should exist and have 4 elements
  --let $assert_cond= [select count(*) from t1] = 4;
  --source include/assert.inc
  --dec $server_count
}

#
#Cleanup
#

DROP TABLE t1;

--source include/rpl_end.inc
