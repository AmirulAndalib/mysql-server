# Multi Master full cluster shutdown
#
# This test aims to validate that if all nodes in the cluster go down and up
# again, Distributed Recovery will work and all node will contain the same
# data in the end, regardless of operations that happened in between.
#

--source include/have_group_replication_plugin.inc
--let $group_replication_group_name= 48530170-7407-11e4-82f8-0800200c9a66
--let $rpl_skip_group_replication_start= 1
--let $rpl_server_count= 3
--source include/master-slave.inc

--echo #
--echo # Setup a new cluster
--echo #

--connection server1
--echo server1
--source include/start_group_replication.inc

--echo # Add some data for recovery

CREATE TABLE t1 (c1 INT NOT NULL PRIMARY KEY) ENGINE=InnoDB;
BEGIN;
INSERT INTO t1 VALUES (1);
INSERT INTO t1 VALUES (2);
COMMIT;
INSERT INTO t1 VALUES (3);

--echo #
--echo # 1) Create a 3 node cluster and verify that it is functional
--echo #

--echo #Add 2 more nodes
--connection server2
--echo server2
--source include/start_group_replication.inc

--connection server3
--echo server3
--source include/start_group_replication.inc

--echo #After recovery all nodes must see 3 other nodes
--let $server_count=3
while ($server_count)
{
  --connection server$server_count
  --let $group_replication_number_of_members= 3
  --source ../inc/rpl_group_replication_wait_for_number_of_members.inc

  --dec $server_count
}

--echo #After recovery all nodes must have the data present in the donor.
--let $server_count=3
while ($server_count)
{
  --connection server$server_count
  --let $assert_text= On all nodes, the table should exist and have 3 elements
  --let $assert_cond= [select count(*) from t1] = 3;
  --source include/assert.inc
  --dec $server_count
}

--echo #
--echo # 2) Shut down all nodes until 0. Add some data in the process.
--echo #

--echo #Stop the node 3
--connection server3
--echo server3

--source include/stop_group_replication.inc

--echo #Add some data to future recoveries and ensure that every node has it
--connection server2
--echo server2
INSERT INTO t1 VALUES (4);
INSERT INTO t1 VALUES (5);

--source include/rpl_sync.inc

--echo #Stop the node 2
--source include/stop_group_replication.inc

--echo #Add some data to future recoveries
--connection server1
--echo server1

INSERT INTO t1 VALUES (6);
INSERT INTO t1 VALUES (7);

--echo #Stop the node 1
--source include/stop_group_replication.inc

--echo #
--echo # 3) Bring up the cluster back to life. At the end, all data must be in
--echo #    all three nodes.
--echo #

--connection server1
--echo server1
--source include/start_group_replication.inc

--connection server2
--echo server2
--source include/start_group_replication.inc

--connection server3
--echo server3
--source include/start_group_replication.inc

--echo #After recovery all nodes must see 3 other nodes
--let $server_count=3
while ($server_count)
{
  --connection server$server_count
  --let $group_replication_number_of_members= 3
  --source ../inc/rpl_group_replication_wait_for_number_of_members.inc

  --dec $server_count
}

--echo #After recovery all nodes must have the data present in the donor.
--let $server_count=3
while ($server_count)
{
  --connection server$server_count
  --let $assert_text= On all nodes, the table should exist and have 7 elements
  --let $assert_cond= [select count(*) from t1] = 7;
  --source include/assert.inc
  --dec $server_count
}

--echo #
--echo # Cleaning up
--echo #

DROP TABLE t1;

--source include/rpl_end.inc
