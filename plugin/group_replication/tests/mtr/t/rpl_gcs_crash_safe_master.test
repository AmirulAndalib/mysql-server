# Test to check the behavior of the GCS nodes when the crash happens on
# a node followed by a recovery on the same.
#
# The general format of the test is :
#
# 1. Crash a node after a transaction but before the commit stage.
# 2. Restart the node.
# 3. Check that no data is replicated on the other nodes.
#

--source include/have_group_replication_plugin.inc
# Don't test this under valgrind, memory leaks will occur
--source include/not_valgrind.inc

SET sql_log_bin=0;
call mtr.add_suppression("Attempting backtrace");
call mtr.add_suppression("allocated tablespace *., old maximum was 0");
call mtr.add_suppression("Error in Log_event::read_log_event()");
call mtr.add_suppression("Buffered warning: Performance schema disabled");
SET sql_log_bin=1;

--let $wait_for_executed_gtid_set= 1
--let $rpl_group_replication= 1
--let $use_gtids= 1

#Restart the first server to erase remains of old rounds
--connection server1
RESET MASTER;
--source include/restart_mysqld.inc
#Reset the servers to delete data from previous rounds
--connection server2
--source include/restart_mysqld.inc

--connection server1

--source include/have_debug.inc
--source include/not_crashrep.inc
--let $group_replication_group_name= `SELECT UUID()`

--let $wait_for_executed_gtid_set=1
--let $rpl_group_replication= 1
--let $use_gtids=1

--connection server1
--source include/start_group_replication.inc

--connection server2
--source include/start_group_replication.inc

--connection server1
--let $old_debug = `select @@global.debug`
CREATE TABLE t1(i INT, a LONGBLOB, PRIMARY KEY(i)) ENGINE=INNODB;

--echo # Test case1: Set DEBUG POINT before binlog to make
--echo # the first node crash for transaction

BEGIN;
--let $rows= 3
WHILE($rows)
{
  --eval INSERT INTO t1 VALUES ($rows, REPEAT('a',2));
  dec $rows;
}

# Write file to make mysql-test-run.pl expect crash and restart
--exec echo "wait" > $MYSQLTEST_VARDIR/tmp/mysqld.1.expect
--disable_query_log
--eval SET SESSION debug=CONCAT(IF(LENGTH('$old_debug') > 0, "$old_debug:", ""), "d,debug,info,enter,return,crash_commit_after_prepare")
--enable_query_log

# Run the crashing query
--error 2013
COMMIT;

--enable_reconnect
--echo # Restart the crashed node
--exec echo "restart" > $MYSQLTEST_VARDIR/tmp/mysqld.1.expect
--source include/wait_until_connected_again.inc
--disable_reconnect

--let $assert_cond= `SELECT COUNT(*) = 0 FROM t1`
--let $assert_text= On server1, test the data will be rolled back after restart.
--source include/assert.inc

# Sync the second node with the first one.
--source include/rpl_sync.inc

--echo # On the second node, test replication will work fine, and the data
--echo # is not replicated
--let $diff_tables= server1:test.t1, server2:test.t1
--source include/diff_tables.inc

--connection server1
--source include/start_group_replication.inc

--let $node_state= query_get_value(SELECT Member_State from performance_schema.replication_group_members, Member_State, 1)
--let $assert_text= The value of member_state should be online
--let $assert_cond= "$node_state" = "ONLINE"
--source include/assert.inc

--connection server2
--let $node_state= query_get_value(SELECT Member_State from performance_schema.replication_group_members, Member_State, 1)
--let $assert_text= The value of member_state should be online
--let $assert_cond= "$node_state" = "ONLINE"
--source include/assert.inc

# Check if the cluster is still working
CREATE TABLE temp (i INT PRIMARY KEY) engine=INNODB;
INSERT INTO temp VALUES (1);

--connection server2
INSERT INTO temp VALUES (2);

--let $wait_condition= select count(*) = 2 from temp;
--source include/wait_condition.inc

--connection server1
--let $wait_condition= select count(*) = 2 from temp;
--source include/wait_condition.inc

DROP TABLE t1;
DROP TABLE temp;
--source include/rpl_sync.inc

--connection server1
--echo # Test case2: Set DEBUG POINT after binlog, and before the data
--echo # is committed to make crash for transaction

--connection server1
CREATE TABLE t1(i INT, a LONGBLOB, PRIMARY KEY(i)) ENGINE=INNODB;

BEGIN;
--let $rows= 3
WHILE($rows)
{
  --eval INSERT INTO t1 VALUES ($rows, REPEAT('a',2));
  --dec $rows
}

# Write file to make mysql-test-run.pl expect crash and restart
--exec echo "wait" > $MYSQLTEST_VARDIR/tmp/mysqld.1.expect
--disable_query_log
--eval SET SESSION debug=CONCAT(IF(LENGTH('$old_debug') > 0, "$old_debug:", ""), "d,debug,info,enter,return,crash_commit_after_log")
--enable_query_log

# Run the crashing query
--error 2013
COMMIT;

--source include/wait_until_disconnected.inc
--enable_reconnect
--echo # Restart the crashed node
--exec echo "restart" > $MYSQLTEST_VARDIR/tmp/mysqld.1.expect
--source include/wait_until_connected_again.inc
--disable_reconnect

--let $assert_cond= `SELECT COUNT(*) = 3 FROM t1`
--let $assert_text= On server1, test the data will be recovered after the server1 restart.
--source include/assert.inc

# Sync the second node with the first one.
--source include/rpl_sync.inc

--echo # On the second node, test that the replication is working fine, and
--echo # data gets replicated here.
--let $diff_tables= server1:test.t1, server2:test.t1
--source include/diff_tables.inc

--connection server1
--source include/start_group_replication.inc

--let $node_state= query_get_value(SELECT Member_State from performance_schema.replication_group_members, Member_State, 1)
--let $assert_text= The value of member_state should be online
--let $assert_cond= "$node_state" = "ONLINE"
--source include/assert.inc

--connection server2
--let $node_state= query_get_value(SELECT Member_State from performance_schema.replication_group_members, Member_State, 1)
--let $assert_text= The value of member_state should be online
--let $assert_cond= "$node_state" = "ONLINE"
--source include/assert.inc

# Check if the cluster is still working
CREATE TABLE temp (i INT PRIMARY KEY) engine=INNODB;
INSERT INTO temp VALUES (1);

--connection server2
INSERT INTO temp VALUES (2);

--let $wait_condition= select count(*) = 2 from temp;
--source include/wait_condition.inc

--connection server1
--let $wait_condition= select count(*) = 2 from temp;
--source include/wait_condition.inc

DROP TABLE t1;
DROP TABLE temp;

--source include/rpl_sync.inc

--connection server1
# Test transaction with large data inserted
CREATE TABLE t1(i INT AUTO_INCREMENT PRIMARY KEY, a LONGBLOB) ENGINE=INNODB;

--echo # Test case3: Set DEBUG POINT after binlogging half of the transaction and
--echo # then crash the node and followed by restarting the node.
set AUTOCOMMIT=0;
BEGIN;
--let $rows= 24
WHILE($rows)
{
  INSERT INTO t1 (a) VALUES ( REPEAT('a',6144));
  --dec $rows
}
# Write file to make mysql-test-run.pl expect crash and restart
--exec echo "wait" > $MYSQLTEST_VARDIR/tmp/mysqld.1.expect
--disable_query_log
--eval SET SESSION debug=CONCAT(IF(LENGTH('$old_debug') > 0, "$old_debug:", ""), "d,debug,info,enter,return,half_binlogged_transaction")
--enable_query_log
# Run the crashing query
--error 2013
COMMIT;

--source include/wait_until_disconnected.inc
--enable_reconnect
--echo # Restart the crashed node
--exec echo "restart" > $MYSQLTEST_VARDIR/tmp/mysqld.1.expect
--source include/wait_until_connected_again.inc
--disable_reconnect

--echo # Test the data will not be recovered successfully
--echo # after the crashed first node restarts.

--let $assert_cond= `SELECT COUNT(*) = 0 FROM t1`
--let $assert_text= On server1, test the data will be recovered after the server1 restart.
--source include/assert.inc

--connection server1
--source include/start_group_replication.inc

# On restarting group replication on the first node it will be getting the transactions
# from a donor and try to get in sync with the other nodes in the cluster.

--let $diff_tables= server1:test.t1, server2:test.t1
--source include/diff_tables.inc

--connection server2
--let $assert_cond= `SELECT COUNT(*) = 24 FROM t1`
--let $assert_text= The table has 24 rows after the restart.
--source include/assert.inc

--let $node_state= query_get_value(SELECT Member_State from performance_schema.replication_group_members, Member_State, 1)
--let $assert_text= The value of member_state should be online
--let $assert_cond= "$node_state" = "ONLINE"
--source include/assert.inc

--connection server2
--let $node_state= query_get_value(SELECT Member_State from performance_schema.replication_group_members, Member_State, 1)
--let $assert_text= The value of member_state should be online
--let $assert_cond= "$node_state" = "ONLINE"
--source include/assert.inc

# Check if the cluster is still running
--connection server1

CREATE TABLE temp (i INT PRIMARY KEY) engine=INNODB;
INSERT INTO temp VALUES (1);

--connection server2
INSERT INTO temp VALUES (2);

--let $wait_condition= select count(*) = 2 from temp;
--source include/wait_condition.inc

--connection server1
--let $wait_condition= select count(*) = 2 from temp;
--source include/wait_condition.inc

--connection server1
DROP TABLE t1;
DROP TABLE temp;
--source include/rpl_sync.inc

--source include/stop_group_replication.inc
--connection server1
--source include/stop_group_replication.inc
