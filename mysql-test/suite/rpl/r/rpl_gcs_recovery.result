include/have_gcs_replication_plugin.inc
#
# Setup a new cluster
#
server1
include/install_gcs_replication.inc
include/start_gcs_replication.inc
# Add some data for recovery
CREATE TABLE t1 (c1 INT NOT NULL PRIMARY KEY) ENGINE=InnoDB;
BEGIN;
INSERT INTO t1 VALUES (1);
INSERT INTO t1 VALUES (2);
COMMIT;
INSERT INTO t1 VALUES (3);
#
# 1) Testing the recovery of a new node
#
#Add 2 more nodes
server2
include/install_gcs_replication.inc
include/start_gcs_replication.inc
server3
include/install_gcs_replication.inc
include/start_gcs_replication.inc
#After recovery all nodes must see 3 other nodes
#After recovery all nodes must have the data present in the donor.
include/assert.inc [On all nodes, the table should exist and have 3 elements]
include/assert.inc [On all nodes, the table should exist and have 3 elements]
include/assert.inc [On all nodes, the table should exist and have 3 elements]
#Test if the cluster responds to requests
server1
INSERT INTO t1 VALUES (4);
server2
INSERT INTO t1 VALUES (5);
server3
#All nodes must have the same GTID set
include/assert.inc [On node 3, all executed GTID should belong to the cluster]
include/assert.inc [On node 2, all executed GTID should belong to the cluster]
include/assert.inc [On node 1, all executed GTID should belong to the cluster]
#Check if the data is the same on joiner and donor
include/diff_tables.inc [server1:test.t1, server2:test.t1]
#
# 2) Testing recovery on a node that left and now returns
#
#Stop the node 1, who never went through recovery
server1
include/stop_gcs_replication.inc
#Add some data to the future donor
server2
INSERT INTO t1 VALUES (6);
server1
include/start_gcs_replication.inc
include/assert.inc [On the recovered node, the table should contain 6 elements]
#Test if the cluster responds to requests
server2
INSERT INTO t1 VALUES (7);
include/diff_tables.inc [server1:test.t1, server2:test.t1]
#
# 3) Testing a second recovery round on a node
#
#Stop the node 2 making it go through recovery again
include/stop_gcs_replication.inc
#Add some data to the future donor
server1
INSERT INTO t1 VALUES (8);
server2
include/start_gcs_replication.inc
include/assert.inc [On the recovered node, the table should contain 8 elements]
#Test if the cluster responds to requests
INSERT INTO t1 VALUES (9);
server1
include/diff_tables.inc [server1:test.t1, server2:test.t1]
#
# Cleaning up
#
DROP TABLE t1;
server3
include/stop_gcs_replication.inc
include/uninstall_gcs_replication.inc
server2
include/stop_gcs_replication.inc
include/uninstall_gcs_replication.inc
server1
include/stop_gcs_replication.inc
include/uninstall_gcs_replication.inc
