# This test intends to prove that nodes can survive reset master commands.
# These commands end up causing the nodes to purge their relay logs
# This test has 2 phases:
#  1) Insert some data and sync both nodes
#  2) Reset both nodes, insert some data on node one
#     Provision node two and start replication
#     Check that all data from phase 1 disappeared and the nodes are in sync


--source include/have_gcs_replication_plugin.inc
--let $gcs_group_name= c7ce5980-0851-11e4-9191-0800200c9a66
--let $rpl_skip_gcs_replication_start= 1
--source include/master-slave.inc

--connection server1
--source include/start_gcs_replication.inc

--echo #Create a table and insert some data
--connection server1
CREATE TABLE t1 (c1 INT NOT NULL PRIMARY KEY) ENGINE=InnoDB;

--connection server2
--source include/start_gcs_replication.inc

--connection server1
INSERT INTO t1 VALUES (1);
INSERT INTO t1 VALUES (2);
INSERT INTO t1 VALUES (3);
INSERT INTO t1 VALUES (4);

--let $sync_slave_connection= server2
--source include/sync_slave_sql_with_master.inc

--echo #Both nodes must have the same GTID set
--let $server_count=2
while ($server_count)
{
  --connection server$server_count
  --let $assert_text= On node $server_count, all executed GTID should belong to the cluster
  --let $assert_cond= "[SELECT @@GLOBAL.GTID_EXECUTED]" = "c7ce5980-0851-11e4-9191-0800200c9a66:1-5";
  --source include/assert.inc
  --dec $server_count
}

DROP TABLE t1;

--let $sync_slave_connection= server2
--source include/sync_slave_sql_with_master.inc

--echo #Stop and reset both nodes

--connection server1
--source include/stop_gcs_replication.inc
RESET MASTER;

--connection server2
--source include/stop_gcs_replication.inc
RESET MASTER;

--connection server1

CREATE TABLE t1 (c1 INT NOT NULL PRIMARY KEY) ENGINE=InnoDB;
INSERT INTO t1 VALUES (1);

--let $server_uuid= `SELECT @@GLOBAL.SERVER_UUID`

--source include/start_gcs_replication.inc

--connection server2

#Fake the node being provisioned.

--disable_result_log
--disable_query_log
eval SET @@SESSION.GTID_NEXT= '$server_uuid:1';
CREATE TABLE t1 (c1 INT NOT NULL PRIMARY KEY) ENGINE=InnoDB;
eval SET @@SESSION.GTID_NEXT= '$server_uuid:2';
INSERT INTO t1 VALUES (1);
--enable_result_log
--enable_query_log

SET GTID_NEXT = 'AUTOMATIC';

#Start the node
--source include/start_gcs_replication.inc

--connection server1

INSERT INTO t1 VALUES (2);
INSERT INTO t1 VALUES (3);

--connection server2

INSERT INTO t1 VALUES (4);
INSERT INTO t1 VALUES (5);

--connection server1

--let $sync_slave_connection= server2
--source include/sync_slave_sql_with_master.inc

--echo #Both nodes should possess the same data

--let $server_count=2
while ($server_count)
{
  --connection server$server_count
  --let $assert_text= On node $server_count, the table should exist and have 5 elements
  --let $assert_cond= [select count(*) from t1] = 5;
  --source include/assert.inc
  --dec $server_count
}

#Wait for the certification thread to kick in
--sleep 60

if(`SELECT "$server_uuid" < "$gcs_group_name"`)
{
--let $wait_condition= SELECT stable_set like "$server_uuid:1-2,%$gcs_group_name:1-4" from performance_schema.replication_node_status;
}
if(`SELECT "$server_uuid" > "$gcs_group_name"`)
{
--let $wait_condition= SELECT stable_set like "$gcs_group_name:1-4,%$server_uuid:1-2" from performance_schema.replication_node_status;
}

--source include/wait_condition.inc
--echo [ The value of stable_set should contain both provisioned and cluster GIIDs ]

--echo #Cleanup

DROP TABLE t1;

--source include/rpl_end.inc
