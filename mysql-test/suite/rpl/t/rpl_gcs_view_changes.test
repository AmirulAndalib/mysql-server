# This test verifies correctness and stability of WL#7332 implementation
# of State Exchange subprotocol.
#
# The structure is as the following.
#
# Z.   Initialization.
#
# I.   The general stability part when servers leave and join randomly.
#      The purpose is to prove no hangup or crashes.
# II.  Two node cluster that demonstrates some (feasible) of Primary component
#      properties.
# III. Proof of view-id monotonicity and cluster-wide consistency.
#
--source include/have_gcs_replication_plugin.inc
--let $gcs_group_name= 8a94f357-aab4-11df-86ab-c80aa9429573
--let $rpl_skip_gcs_replication_start= 1
--let $rpl_server_count= 4
--source include/master-slave.inc

#
# a counter to be used with source include/rpl_gcs_error_out.inc
#
--let $gcs_error_connection_number= 4

#
# First prepare four nodes and manually create a table on each.
#
--connection server1

SET SESSION sql_log_bin= 0;
call mtr.add_suppression("could not form the cluster");
SET SESSION sql_log_bin= 1;

if (`select view_id <> 0 from performance_schema.replication_connection_status`)
{
    --echo incorrect non-zero view_id when the Node is never started.
    --source include/rpl_gcs_error_out.inc
    --die
}

--connection server4

CREATE TABLE t1 (c1 INT NOT NULL PRIMARY KEY) ENGINE=InnoDB;

#
# Part I.
#
# Restart four nodes few times to prove general stability. It should
# be done carefully to not destroy the cluster by occasional
# withdrawal more than cluster.number_of_nodes / 2 - 1 nodes at a
# time, that is not more than 1 in the condition of this test.
# The test implements the following transition: 4 -> 3 -> 2 -> 3 -> 4.
#

--disable_query_log
--disable_result_log
--let $include_silent= 1

--let $s_cnt=4
while($s_cnt)
{
    --connection server$s_cnt
    --source include/start_gcs_replication.inc

    --dec $s_cnt
}
let $wait_condition= select number_of_nodes = 4 from performance_schema.replication_connection_status;
--source include/wait_condition.inc

--let $restart_cnt= 10
while($restart_cnt)
{
    --let $s_id1=`select 1 + floor(rand()*100 % 4)`

    --connection server$s_id1

    STOP GCS_REPLICATION;

    --let $s_id2= $s_id1
    while ($s_id2 == $s_id1)
    {
	--let $s_id2=`select 1 + floor(rand()*100 % 4)`
    }
    --connection server$s_id2

    #
    # Here is the care point: don't go stopping the 2nd node until
    # it's proved the previous one is out of the view.
    #
    let $wait_condition= select number_of_nodes = 3 from performance_schema.replication_connection_status;
    --source include/wait_condition.inc

    STOP GCS_REPLICATION;

    --connection server$s_id1
    START GCS_REPLICATION;

    --connection server$s_id2
    --source include/start_gcs_replication.inc

    let $wait_condition= select number_of_nodes = 4 from performance_schema.replication_connection_status;
    --source include/wait_condition.inc

    dec $restart_cnt;
}
--let $include_silent= 0
--enable_result_log
--enable_query_log

#Check if all nodes are online
--let $s_cnt=4
while ($s_cnt)
{
    --connection server$s_cnt
    let $wait_condition= select node_state="ONLINE" from performance_schema.replication_connection_nodes;
    --source include/wait_condition.inc
    --dec $s_cnt
}

#
# Part II.
#
# Form the 2 node cluster and verify view-id on each node, must be equal.
# At forming make sure two extra members exits one by one so the quorum
# is not get lost.
#

--connection server3
--source include/stop_gcs_replication.inc

--connection server4
let $wait_condition= select number_of_nodes = 3 from performance_schema.replication_connection_status;
--source include/wait_condition.inc

--source include/stop_gcs_replication.inc

--connection server1
let $wait_condition= select number_of_nodes = 2 from performance_schema.replication_connection_status;
--source include/wait_condition.inc

# end of forming.

--connection server1
INSERT INTO t1 SET c1=1;

--connection server2

#
# Success in getting expected value indicates the Primary Component
# is installed. Let's verify the # of nodes is as PS says
#
let $count= 1;
let $table= t1;
--source include/wait_until_rows_count.inc

if (`SELECT number_of_nodes <> 2 from performance_schema.replication_connection_status`)
{
    --echo Unexpected cluster membership.
    --source include/rpl_gcs_error_out.inc
    --die
}

# By above the Primary Component of two group members installation is proved.
# Let's check out view-id values.
--connection server1
--let $view_id_server1=`select view_id from performance_schema.replication_connection_status`

--connection server2
--let $view_id_server2=`select view_id from performance_schema.replication_connection_status`

if ($view_id_server1 == 0)
{
    --echo view_id must be non-zero at this point.
    --source include/rpl_gcs_error_out.inc
    --die
}
if ($view_id_server1 != $view_id_server2)
{
    --echo inconsistent view_id:s: $view_id_server1 != $view_id_server2
    --source include/rpl_gcs_error_out.inc
    --die
}

#
# Verify the Primary Component property of aborting transactions
# whose originator nodes left the Primary Component.
# The policy is tentative, it might (should) be a subject of
# softer treatment, e.g wait with aborting till a timeout elapses.
# Yet it's tested 'cos it's there.
#
--connection server1
BEGIN;
INSERT INTO t1 SET c1=2;
# Split the cluster and see that the left alone server1 won't make
# Primary Component out of itself.

--connection server2
--source include/stop_gcs_replication.inc

# TODO: implement COMMIT instead of the coded rollback
--connection server1
rollback;

#
# Part III.
#
# Form the four node cluster, check up monotonic and cluster consistent view-id.
#
--connection server2
--source include/start_gcs_replication.inc
#
# TODO: fix the acquire_ownership bug. Can't executed anything - hitting the above assert.
# INSERT INTO t1 SET c1=2;
--connection server3
--source include/start_gcs_replication.inc

--connection server4
--source include/start_gcs_replication.inc

--let $s_cnt=4
while ($s_cnt)
{
    --connection server$s_cnt
    let $wait_condition= select number_of_nodes = 4 from performance_schema.replication_connection_status;
    --source include/wait_condition.inc

    --dec $s_cnt
}
#
#  a proof of view-id is monotonic
#
--let $view_id_now=`select view_id from performance_schema.replication_connection_status`
if ($view_id_now <= $view_id_server1)
{
    --echo Unexpected non-increased view-id value.
    --source include/rpl_gcs_error_out.inc
    --die
}

#
#  view-id cross cluster consistency
#
--let $s_cnt=3
while ($s_cnt)
{
    --connection server$s_cnt
    --let $view_id_now_s=`select view_id from performance_schema.replication_connection_status`
    if ($view_id_now != $view_id_now_s)
    {
	   --echo The 4th node view inconsistent with that of $s_cnt.
	   --source include/rpl_gcs_error_out.inc
	   --die
    }
    dec $s_cnt;
}


#
#  Cleanup
#
DROP TABLE t1;

--source include/rpl_end.inc
